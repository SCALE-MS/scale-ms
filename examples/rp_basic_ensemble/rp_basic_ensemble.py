"""Prepare and run an ensemble of trajectories through RADICAL Pilot.

We will rely on RADICAL Pilot to manage resource allocation. For simplicity,
this example assumes that the local.localhost RP resource is configured appropriately,
and that the "local" access scheme is sufficient.

This example is tested with thread-MPI GROMACS. MPI parallelism in scalems.radical
tasks is under investigation.

The *cores-per-sim* must not exceed the cores available to the pilot.

For example, for an ensemble of size 2, and 2 cores per simulation,
activate your gmxapi Python virtual environment.

For thread-MPI gromacs, run::

    python rp_basic_ensemble.py \
        --resource local.localhost \
        --access ssh \
        --venv $VIRTUAL_ENV \
        --pilot-option cores=4 \
        --procs-per-sim 1 \
        --threads-per-sim 2 \
        --mdrun-arg nt 2 \
        --mdrun-arg ntomp 1 \
        --size 2 \
        --mdrun-arg maxh 1.0

or::

    python rp_basic_ensemble.py \
        --resource local.localhost \
        --access ssh \
        --venv $VIRTUAL_ENV \
        --pilot-option cores=4 \
        --procs-per-sim 1 \
        --threads-per-sim 2 \
        --mdrun-arg nt 2 \
        --size 2 \
        --mdrun-arg maxh 1.0

For MPI GROMACS::

    python rp_basic_ensemble.py \
        --resource local.localhost \
        --access ssh \
        --venv $VIRTUAL_ENV \
        --pilot-option cores=4 \
        --procs-per-sim 2 \
        --size 2 \
        --mdrun-arg maxh 1.0

Note that RP access schemes based on forking the Python interpreter may not work
right with MPI-enabled tasks. Check the resource definition and prefer an access
scheme that uses ``ssh`` or a job management system, like ``slurm``.

"""
from __future__ import annotations

import asyncio
import argparse
import collections.abc
import logging
import os
import pathlib
import sys
import typing
import urllib.parse
from dataclasses import dataclass
from dataclasses import field
from pathlib import Path
from typing import Optional
from typing import Protocol
from typing import Sequence
from typing import Union

import gmxapi
import gmxapi.abc

import scalems
import scalems.call
import scalems.execution
import scalems.radical
import scalems.radical.task
import scalems.workflow
from scalems.exceptions import MissingImplementationError

ResultType = typing.TypeVar("ResultType")
T = typing.TypeVar("T")


class Future(Protocol[ResultType]):
    """Defines the interface of a Task output handle."""

    def result(self) -> ResultType:
        ...


TaskInput = Union[Future[T], T]


@dataclass
class _ParsedArgs:
    executable: str
    arguments: typing.List[str] = field(default_factory=list)
    input_files: typing.Dict[str, typing.Union[Future[str], str]] = field(default_factory=dict)
    output_files: typing.Dict[str, str] = field(default_factory=dict)


class PlaceHolder(typing.Generic[T]):
    """Annotation type allowing for additional processing by the framework."""


CommandLineArgvType = Sequence[typing.Union[str, pathlib.Path, Future[str], PlaceHolder[str]]]

SupportsEnsemble = Union[TaskInput[T], Sequence[TaskInput[T]]]


@dataclass(frozen=True)
class OutputFilePlaceholder(PlaceHolder[str]):
    """Annotation type declaring a file that will be generated by a Task."""

    filename: typing.Optional[str]
    label: typing.Optional[str]
    suffix: typing.Optional[str]

    # Note that the initial definition is for a stateless object. We could consider
    # a stateful object that acquires a reference to the Future that will produce
    # the output file:
    # future: Future = None  # Reference to the Future that delivers the output file, once established.
    #
    # We could also add behaviors to allow OutputFilePlaceholder to learn its
    # Path and deliver the appropriate string when referenced. However, we will
    # try to keep things simple while we can. We will rely on separate machinery
    # to replace instances of OutputFilePlaceholder with appropriate values
    # where needed.


class _MonotonicInteger:
    __value = -1

    @staticmethod
    def get():
        _MonotonicInteger.__value += 1
        return _MonotonicInteger.__value


class MDRunResult(typing.TypedDict):
    """Return type for the MDRun Command."""

    trajectory: str
    directory: str


async def main(
    *,
    script_config: argparse.Namespace,
    runtime_config: scalems.radical.runtime_configuration.RuntimeConfiguration,
    manager: scalems.workflow.WorkflowManager,
    label: str,
) -> tuple[MDRunResult]:
    """Gromacs simulation on ensemble input

    Call the GROMACS MD preprocessor to create a simulation input file. Declare an
    ensemble simulation workflow starting from the single input file.

    Args:
        runtime_config: runtime configuration for scalems.radical backend.
        script_config: namespace or named tuple with configuration values. (Refer to the argparse parser.)
        manager: an active scalems WorkflowManager instance.

    Returns:
        Trajectory output. (list, if ensemble simulation)
    """
    import gmxapi as gmx

    input_dir: Path = script_config.inputs
    ensemble_size: int = script_config.size
    mdrun_args = {opt[0]: " ".join(opt[1:]) for opt in script_config.mdrun_args}

    commandline = [
        gmx.commandline.cli_executable(),
        "pdb2gmx",
        "-ff",
        "amber99sb-ildn",
        "-water",
        "tip3p",
        "-f",
        os.path.join(input_dir, "start0.pdb"),
        "-p",
        output_file("topol.top"),
        "-i",
        output_file("posre.itp"),
        "-o",
        output_file("conf.gro"),
    ]
    make_top = executable(commandline)

    # make array of inputs
    commandline = [
        gmx.commandline.cli_executable(),
        "grompp",
        "-f",
        os.path.join(input_dir, "grompp.mdp"),
        # TODO: executable task output proxy
        # '-c', make_top.output_file['conf.gro'],
        # '-p', make_top.output_file['topol.top'],
        "-c",
        make_top.output.file["-o"],
        "-p",
        make_top.output.file["-p"],
        "-o",
        output_file("run.tpr", label="simulation_input"),
    ]
    grompp = executable([commandline])

    # TODO: executable task output proxy
    # tpr_input = grompp.output_file['simulation_input']
    tpr_input = grompp.output.file["-o"].result()

    # session: scalems.radical.runtime.RPDispatchingExecutor
    # async with scalems.execution.dispatch(
    #     manager, executor_factory=scalems.radical.executor_factory, params=runtime_config
    # ) as dispatcher:
    #     simulations = tuple(
    #         MDRun(
    #             tpr_input,
    #             runtime_args=mdrun_args,
    #             task_ranks=script_config.procs_per_sim,
    #             task_cores_per_rank=script_config.threads_per_sim,
    #             manager=manager,
    #             dispatcher=dispatcher,
    #             label=f"{label}-{i}",
    #         )
    #         for i in range(ensemble_size)
    #     )
    #     futures = tuple(asyncio.create_task(md.result(), name=md.label) for md in simulations)
    #     for future in futures:
    #         future.add_done_callback(lambda x: print(f"Task done: {repr(x)}."))
    #     return await asyncio.gather(*futures)

    workflow = scalems.radical.workflow_manager(loop)
    # Explicitly activate and deactivate a managed workflow with filesystem-backed metadata,
    # providing a clear point for filesystem flushing before interpreter shutdown.
    with scalems.workflow.scope(workflow, close_on_exit=True):
        # The scalems.workflow module now tracks the currently active WorkflowManager.

        # Explicitly select the RP back-end and configure the Pilot session.
        with scalems.radical.runtime.launch(workflow, runtime_config) as runtime_context:
            # The scalems.execution module now tracks the default RuntimeManager,
            # bound to the provided WorkflowManager.

            # Provision Raptor with known (or knowable) Worker resource requirements.
            # Defines a scope for tasks with mostly homogeneous runtime requirements,
            # and allows for an "ensemble scope" in which tasks and data can be more tightly and clearly coupled,
            # or in which data and compute placement could be better optimized.
            async with scalems.execution.executor(
                runtime_context,
                worker_requirements=[{"ranks": config.procs_per_sim, "cores_per_rank": config.threads_per_sim}]
                * number_of_workers,
                task_requirements={"ranks": config.procs_per_sim, "cores_per_rank": config.threads_per_sim},
            ):
                # Assuming `tpr_input` is an array-like set of inputs that we want to launch simulations for, as resources allow.
                simulations: scalems.Task = scalems.map(functools.partial(MDRun, runtime_args=mdrun_args), tpr_input)
                # task3 has a snapshot (`contextvars.copy()`) of any module-global variables needed
                # to support asynchronous scheduling and Future fulfillment outside of the `with` block,
                # allowing us to use this context manager hierarchy for scripting interface / program scoping
                # without unnecessarily strong coupling to the underlying runtime management.
                for i, task in enumerate(simulations):
                    task.label = f"{label}-{i}"
                    task.add_done_callback(lambda x: print(f"Task done: {repr(x)}."))
            # Task handles must still be valid outside of the executor context manager in order to schedule
            # chains of tasks with different resource requirements. Unlike concurrent.futures.Executor, then,
            # `ScalemsExecutor.__exit__()` should probably default to `self.shutdown(wait=False)`.
            return await asyncio.gather(*simulations)


class MDRun:
    """Instance of a simulation Command."""

    def __init__(
        self,
        input,
        *,
        label: str,
        runtime_args: dict,
        task_ranks: int,
        task_cores_per_rank: int,
        manager: scalems.workflow.WorkflowManager,
        dispatcher: scalems.radical.runtime.RPDispatchingExecutor,
    ):
        self.label = label

        # TODO: Manage input file staging so we don't have to assume localhost.
        args = (input,)
        kwargs = {"runtime_args": runtime_args.copy()}
        requirements = {
            "ranks": task_ranks,
            "cores_per_rank": task_cores_per_rank,
            "threading_type": "OpenMP",
        }
        task_uid = label
        self._call_handle: asyncio.Task[scalems.call._Subprocess] = asyncio.create_task(
            scalems.call.function_call_to_subprocess(
                func=self._func,
                label=task_uid,
                args=args,
                kwargs=kwargs,
                manager=manager,
                requirements=requirements,
            )
        )
        self._dispatcher = dispatcher

    @staticmethod
    def _func(simulation_input, *, runtime_args=None):
        """Task implementation."""
        from gmxapi import mdrun

        if runtime_args is None:
            runtime_args = {}
        md = mdrun(simulation_input, runtime_args=runtime_args)
        return md.output.trajectory.result()

    async def result(self) -> MDRunResult:
        """Deliver the results of the simulation Command."""
        # Wait for input preparation
        call_handle = await self._call_handle
        rp_task_result_future = asyncio.create_task(
            scalems.radical.task.subprocess_to_rp_task(call_handle, dispatcher=self._dispatcher)
        )
        # Wait for submission and completion
        rp_task_result = await rp_task_result_future
        result_future = asyncio.create_task(
            scalems.radical.task.wrapped_function_result_from_rp_task(call_handle, rp_task_result)
        )
        # Wait for results staging.
        result: scalems.call.CallResult = await result_future
        # Note that the return_value is the trajectory path in the RP-managed Task directory.
        # TODO: stage trajectory file, explicitly?
        return {"trajectory": result.return_value, "directory": result.directory}


def _parse_argv(argv: Sequence[typing.Union[str, Future[str], PlaceHolder[str]]]):
    """Process a sequence into positional arguments and I/O flag mappings.

    Bare strings are either positional arguments or input/output flags.

    Input flags are followed by one or more Future arguments, which establish
    data flow dependencies.

    Output flags are followed by one or more placeholders to establish named
    outputs for the task.
    """
    # For gmxapi.commandline_operation, positional arguments must come before input/output
    # file arguments.
    if isinstance(argv, (str, bytes)) or not isinstance(argv, collections.abc.Sequence) or len(argv) == 0:
        raise TypeError("argv must be the array of command line arguments (including the executable).")
    num_args = len(argv)
    executable = str(argv[0])

    if num_args == 1:
        return _ParsedArgs(executable=executable)

    # Gather positional arguments.
    assert num_args > 1
    previous_arg = argv[1]
    if not isinstance(previous_arg, (str, pathlib.Path)):
        raise ValueError(
            "Unsupported command line syntax. Basic strings must be used for positional arguments. "
            'Input and output files must be provided with a "flag" argument for identification.'
        )
    arguments = []
    input_files: typing.Dict[str, typing.Union[Future[str], str]] = {}
    output_files: typing.Dict[str, str] = {}
    flag: Optional[str] = None
    i = 1
    # Scan for input/output flags by looking for non-string flag arguments.
    for i, arg in enumerate(argv[2:], start=2):
        if isinstance(arg, (str, pathlib.Path)):
            arguments.append(str(previous_arg))
            previous_arg = arg
        elif isinstance(arg, gmxapi.abc.Future):
            flag = previous_arg
            input_files[flag] = arg
            break
        elif isinstance(arg, OutputFilePlaceholder):
            flag = previous_arg
            output_files[flag] = get_path(arg)
            break
        else:
            raise ValueError(f"Invalid element in argv: {repr(arg)}")

    if flag is None:
        # No non-positional arguments found. Handle the final argument and return.
        arguments.append(previous_arg)
        return _ParsedArgs(executable=executable, arguments=arguments)
    else:
        # input/output arguments found. argv[i-1] is a flag and argv[i] was its
        # (first) argument.
        previous_arg = argv[i]

    # Extend the first I/O flag (discovered above), if necessary,
    # and gather any remaining I/O flags and file arguments.
    for arg in argv[i + 1 :]:
        if isinstance(arg, str):
            # Check for empty file lists before discarding the previous flag.
            if isinstance(previous_arg, str):
                assert previous_arg is flag
                raise ValueError(
                    f"Flags {repr(previous_arg)} and {repr(arg)} appeared in sequence "
                    f"where input/output flags and arguments are expected."
                )
            # We don't know yet whether it is an input or output flag.
            # We will infer on a later iteration from its argument type.
            flag = arg
            if flag in input_files or flag in output_files:
                raise ValueError(f"Duplicated input/output file flags not supported: {flag}")
        elif isinstance(arg, OutputFilePlaceholder):
            if flag in input_files:
                raise ValueError(f"Output file {arg} provided to {flag}, but {flag} is an input file flag.")
            # gmxapi.commandline_operation currently requires one filename per flag.
            # if flag not in output_files:
            #     output_files[flag] = []
            # output_files[flag].append(get_path(arg))
            if flag in output_files:
                raise ValueError(
                    f"Output file {arg} provided to {flag}, but {flag} is already set to " f"{output_files[flag]}."
                )
            else:
                output_files[flag] = get_path(arg)
        elif isinstance(arg, gmxapi.abc.Future):
            if flag in output_files:
                raise ValueError(f"Input file provided to {flag}, but {flag} is an output file flag.")
            # gmxapi.commandline_operation currently requires one filename per flag.
            # if flag not in input_files:
            #     input_files[flag] = []
            # input_files[flag].append(arg)
            if flag in input_files:
                raise ValueError(
                    f"Input file {arg} provided to {flag}, but {flag} is already set to " f"{input_files[flag]}."
                )
            else:
                input_files[flag] = arg
        else:
            raise ValueError(f"Cannot process input/output file argument {repr(arg)}.")
        previous_arg = arg

    if flag is previous_arg:
        raise ValueError(f"Got flag {flag} with no arguments.")

    return _ParsedArgs(executable=executable, arguments=arguments, input_files=input_files, output_files=output_files)


def executable(
    argv: typing.Union[CommandLineArgvType, Sequence[CommandLineArgvType]],
    *,
    resources: Optional[SupportsEnsemble[dict]] = None,
    inputs: Optional[SupportsEnsemble[Sequence]] = None,
    outputs: Optional[SupportsEnsemble[Sequence]] = None,
    stdin: Optional[SupportsEnsemble[str]] = None,
    env: Optional[SupportsEnsemble[dict]] = None,
):
    """Wrap a command line executable to produce a Task.

    Arguments:
        argv: command line argument array, beginning with the executable.
        resources: named runtime resource requirements for the executable Task.
        inputs: enumerate implicit inputs (beyond command line arguments).
        outputs: enumerate implicit outputs (beyond *output_file* elements in *argv*).
        stdin: A string (including line breaks) to be sent to the executable.
        env: optionally substitute a given dictionary instead of inheriting
             environment variables from the execution environment or launch method.

    The first element of *argv* is assumed to be the executable. If the first
    element of *argv* is not a Path or a suitable representation of a filesystem
    path, but is a Sequence, *argv* is assumed to describe an array of Tasks.
    Elements of *argv* may be strings (compatible with filesystem encoding),
    Futures, or placeholders (see `output_file()`).

    Returns:
        Task handle, providing one or more Future outputs.

    See also:
        :py:func:`output_file()`

    Notes:
        Initial implementation is a wrapper for
        :py:func:`gmxapi.commandline_operation` which incurs some additional
        limitations.
         * All input and output files must occur as command line options.
         * *resources* is not yet supported.
         * tasks are executed sequentially to avoid resource conflicts.
         * *env* must be overwritten for MPI-aware executables when the script
           is executed as an MPI task (e.g. ``mpiexec ... python -m mpi4py myscript.py``)
    """
    # Initial implementation wraps `gmxapi.commandline_operation`.
    if resources is not None:
        raise MissingImplementationError("resources argument is not yet supported.")
    if inputs is not None:
        # We need to consider what to do when entries in *inputs* duplicate elements of *argv*.
        raise MissingImplementationError("inputs argument is not yet supported.")
    if outputs is not None:
        # We need to consider what to do when entries in *outputs* duplicate elements of *argv*.
        raise MissingImplementationError("outputs argument is not yet supported.")

    if not isinstance(argv, collections.abc.Sequence) or isinstance(argv, (str, bytes)) or len(argv) == 0:
        raise TypeError(f"argv must be a sequence of command line arguments. Got {type(argv)}.")

    if stdin is not None and not isinstance(stdin, (str, Future)):
        if (
            not isinstance(stdin, collections.abc.Sequence)
            or len(stdin) == 0
            or not all(isinstance(element, str) for element in stdin)
        ):
            raise TypeError("If provided, stdin must be a string or (for ensemble input) a sequence of strings.")

    # Check for ensemble input and parse argv.
    if isinstance(argv[0], collections.abc.Sequence) and not isinstance(argv[0], (str, bytes)):
        # A list of argv sequences is valid ensemble input as long as they have compatible input and output
        # edges.
        kwargs_list = [_parse_argv(element) for element in argv]
        _executable = kwargs_list[0].executable
        if not all(_executable == kwargs.executable for kwargs in kwargs_list):
            # This is a constraint of gmxapi.commandline_operation. It does not necessarily
            # need to be preserved in scalems.executable, but this is a potential discussion point.
            raise ValueError("Ensemble command line operations must use the same command line tool.")
        _arguments = [kwargs.arguments for kwargs in kwargs_list]
        _input_files = [kwargs.input_files for kwargs in kwargs_list]
        _output_files = [kwargs.output_files for kwargs in kwargs_list]
    else:
        kwargs = _parse_argv(argv)
        _executable = kwargs.executable
        _arguments = kwargs.arguments
        _input_files = kwargs.input_files
        _output_files = kwargs.output_files

    # Infer input_files and output_files
    cmd = gmxapi.commandline_operation(
        executable=_executable,
        arguments=_arguments,
        input_files=_input_files,
        output_files=_output_files,
        stdin=stdin,
        env=env,
    )

    # TODO: Apply an adapter for the intended interface.
    # return gmxapi_adapter(cmd, ...)
    return cmd


def get_path(placeholder: OutputFilePlaceholder) -> str:
    """

    Returns:
        str: an appropriate filename
    """
    if placeholder.filename:
        path: str = placeholder.filename
    else:
        if placeholder.label:
            path = placeholder.label
        else:
            path = "output"
        path += str(_MonotonicInteger.get())
        if placeholder.suffix:
            path += placeholder.suffix
    return path


def output_file(filename: typing.Union[str, Future] = None, *, label: str = None, suffix: str = None):
    """Declare an output file.

    When used in a string context (e.g. a `executable` *argv* element),
    the *output_file* placeholder is automatically converted to the full
    output file path.

    If *filename* is not provided, a suitable filename is generated (optionally
    constrained by *suffix*, if provided).

    If *filename* is a relative path, it will be expanded relative to the Task
    working directory.

    If *filename* already exists, behavior is determined by the tool supporting
    the Task.

    If *label* is unspecified, the *filename* is used as the output key.
    """
    if filename is not None and not isinstance(filename, str):
        try:
            # noinspection PyTypeChecker
            filename = str(os.fspath(filename), encoding="utf8")
        except TypeError:
            raise ValueError("filename must be a str or None.")
    if label is not None and not isinstance(label, str):
        raise ValueError("label must be a str or None.")
    if suffix is not None and not isinstance(suffix, str):
        raise ValueError("suffix must be a str or None.")
    if suffix and filename is not None:
        raise ValueError("A suffix may only be requested when *filename* is unspecified.")
    return OutputFilePlaceholder(filename=filename, label=label, suffix=suffix)


if __name__ == "__main__":
    import scalems.radical

    # Set up a command line argument processor for our script.
    # Inherit from the backend parser so that `parse_known_args` can handle positional arguments the way we want.
    parser = argparse.ArgumentParser(
        parents=[scalems.radical.runtime_configuration.parser()],
        add_help=True,
        description="Warning: The automatically generated usage information is not quite right, "
        "pending normalization with the scalems backend invocation. "
        "Refer to the docstring in the file for details.",
    )

    parser.add_argument(
        "--procs-per-sim",
        type=int,
        default=1,
        help="Processes (MPI ranks) per simulation task. (default: %(default)s)",
    )
    parser.add_argument(
        "--threads-per-sim",
        type=int,
        default=1,
        help="OMP_NUM_THREADS in simulation processes. (default: %(default)s)",
    )
    # I wasn't able to quickly find a user-friendly way to process arbitrarily multi-valued
    # arguments that, themselves, look like arguments that would terminate a `nargs="*"` stream.
    parser.add_argument(
        "--mdrun-arg",
        dest="mdrun_args",
        action="append",
        default=[],
        nargs="*",
        help="Option flag (with the leading '-' removed)"
        " and value(s) to be passed to the GROMACS simulator. Use once per option.",
        metavar="OPTION VAL1 [VAL2]",
    )
    parser.add_argument(
        "--inputs",
        type=Path,
        default=Path(__file__).resolve().parent.parent.parent / "testdata" / "fs-peptide",
        help="Directory containing fs-peptide input files. (default: %(default)s)",
    )
    # parser.add_argument('--log-level', default='ERROR',
    #     help='Minimum log level to handle for the Python "logging" module. (See '
    #          'https://docs.python.org/3/library/logging.html#logging-levels)')
    parser.add_argument(
        "--size",
        type=int,
        default=1,
        help="Ensemble size: number of parallel pipelines. (default: %(default)s)",
    )

    # Work around some quirks: we are using the parser that normally assumes the
    # backend from the command line. We can switch back to the `-m scalems.radical`
    # style invocation when we have some more updated UI tools
    # (e.g. when scalems.wait has been updated) and `main` doesn't have to be a coroutine.
    sys.argv.insert(0, __file__)

    # Handle command line invocation.
    script_config, argv = parser.parse_known_args()
    runtime_configuration = scalems.radical.runtime_configuration.configuration(argv)

    # Configure logging module before using tools that use it.
    level = None
    debug = False
    if script_config.log_level is not None:
        level = logging.getLevelName(script_config.log_level.upper())
        debug = level <= logging.DEBUG
    if level is not None:
        character_stream = logging.StreamHandler()
        character_stream.setLevel(level)
        formatter = logging.Formatter("%(asctime)s-%(name)s:%(lineno)d-%(levelname)s - %(message)s")
        character_stream.setFormatter(formatter)
        logging.basicConfig(level=level, handlers=[character_stream])

    logging.info(f"Input directory set to {script_config.inputs}.")

    # Call the main work.
    manager = scalems.radical.workflow_manager(asyncio.get_event_loop())
    with scalems.workflow.scope(manager, close_on_exit=True):
        md_outputs = asyncio.run(
            main(
                script_config=script_config,
                runtime_config=runtime_configuration,
                manager=manager,
                label="rp-basic-ensemble",
            ),
            debug=debug,
        )

    trajectory = [md["trajectory"] for md in md_outputs]
    directory = [md["directory"] for md in md_outputs]

    # Write output file manifests
    with open("zipfiles.manifest", "w") as fh:
        for url in directory:
            # These paths are in the "local" filestore (`./scalems_X_Y/`)
            fh.write(urllib.parse.urlparse(url).path + "\n")
    with open("trajectories.manifest", "w") as fh:
        for path in trajectory:
            # These paths are in the execution environment.
            fh.write(str(path) + "\n")

    for i, out in enumerate(zip(trajectory, directory)):
        print(f"Trajectory {i}: {out[0]}. Directory archive (zip file) {i}: {out[1]}")
    # with zipfile.ZipFile('/home/rp/tmp/scalems_0_0/fb714a2170e74e479ce2c2f9e92d66ace4f0941cd5413643c7ceebf82783bf21', 'r') as myzip:
    #     ...     myzip.namelist()
